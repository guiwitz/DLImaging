{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AI1uO_bdYDRH"
   },
   "source": [
    "# 4. Simple neural net with PyTorch\n",
    "\n",
    "Neural networks can be programmed on different levels depending on how much one needs to customize either the architecture or the training pattern. When dealing with more complex NN we will use a higher-level package (Lightning, see [Chapter 8](08-Lightning.ipynb)) which will spare us some \"manual\" work. However, in order to understand all steps involved in designing a DL algorithm, we will first use basic PyTroch. In this notebook we will see howe we can *design* a NN while in [the next notebook](05-Training.ipynb) we will se how to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py', 'check_colab.py')\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer from torch.nn\n",
    "\n",
    "We will see in this notebook how to create a simple neural network for image classification using the most general mechanism in PyTorch which are modules. All module-related objects and functions are contained in ```torch.nn```. As the name indicates, these modules are really *modular* in the sense that they are indpendent parts of networks that can be combined together. Mostly we will use single modules containing the multiple layers of our architecture, but we will also see examples where multiple modules are assembled.\n",
    "\n",
    "A module is essentially an assemble of multiple other modules, and many of these are provided \"pre-made\" for us also available in ```torch.nn```. For example in the simple network used here we will only use linear layers. Before we create an actual network let's see how layers work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4633,
     "status": "ok",
     "timestamp": 1610065427087,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "YL9Tx6P0YDRM"
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat now a linear layer that takes a vector of size 5 as input and ouptuts a vector of size 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer = nn.Linear(in_features=5, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single layer already has parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1192, -0.4216,  0.1800,  0.2818,  0.1306],\n",
       "         [-0.0854,  0.4079,  0.1834, -0.1045,  0.4207],\n",
       "         [-0.1844,  0.3427, -0.1615, -0.0069, -0.3641],\n",
       "         [ 0.2074,  0.0595, -0.3610,  0.1240,  0.0372],\n",
       "         [ 0.3725,  0.4168,  0.1345, -0.3373, -0.3602],\n",
       "         [-0.0335, -0.0953,  0.2978, -0.3225, -0.2697],\n",
       "         [-0.3293, -0.0243,  0.1906,  0.3574, -0.0574],\n",
       "         [-0.3406, -0.3446,  0.1879,  0.1895, -0.3135],\n",
       "         [ 0.3006, -0.4068,  0.1791,  0.2753, -0.1134],\n",
       "         [ 0.2713,  0.0084, -0.2138,  0.2711,  0.3573]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2798, -0.0968, -0.1219, -0.2079, -0.1955, -0.1343, -0.1134, -0.1071,\n",
       "         -0.3874, -0.3514], requires_grad=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lin_layer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have indeed a 5x10 matrix plus the bias term of size 10. When we later assemble multiple modules, we will see the same time of output but with many more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Kp9kzeYDRR"
   },
   "source": [
    "## Passing an input\n",
    "\n",
    "Our layer takes a vector as an input so let's try to creat a vector of size 5 and pass it through the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1610065446596,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "BrpgmCV9YDRS"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1610065481351,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "WgH4jbSpYDRS",
    "outputId": "1db57c13-14cc-458e-d140-5b7880fdb913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 12, 22, 91, 44])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvector = np.random.randint(0,100,5)\n",
    "myvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXXbN8u2YDRS"
   },
   "source": [
    "No we we pass it to ```lin_layer```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "error",
     "timestamp": 1610065483170,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "lSXkG3RfYDRS",
    "outputId": "5473b837-42ca-48b2-e573-3031442deb95",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlin_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyvector\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "lin_layer(myvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR4xQZciYDRS"
   },
   "source": [
    "Of course we get an error, as the layer doesn't expect a Numpy array but a PyTorch tensor! We can have a look at what PyTorch tensors are in the [03-Tensors](03-Tensors.ipynb) notebook and come back here later.\n",
    "\n",
    "Now we can turn our Numpy array into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1610065485142,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "lNz6N6ZgYDRT"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1610065485503,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "eHX1lbiCYDRT",
    "outputId": "63f442c7-7567-4469-f3de-e40c6e9dda7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42, 12, 22, 91, 44])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor = torch.tensor(myvector)\n",
    "mytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "error",
     "timestamp": 1610065486424,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "m_-lAb5SYDRT",
    "outputId": "f98b9767-5d07-433f-e7e7-b316845b8663",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlin_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmytensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CASImaging/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "lin_layer(mytensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV6W7QoJYDRT"
   },
   "source": [
    "Again an error! The error can be a bit misleading. The whole problem is that the weights in the layer are defined by default as ```float32```. Hence the input should match this but we passed an 64 bits integer which creates a conflict. Let's adjust the type of our tensor. We can either modify the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1610065490239,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "eCYVteuRYDRT"
   },
   "outputs": [],
   "source": [
    "mytensor_float = mytensor.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1610065491111,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "MnABMWbkYDRU",
    "outputId": "aed5ad21-e85c-43a9-ff03-84bd5e08bb75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42., 12., 22., 91., 44.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1610065491323,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "9aj1UP4ZYDRU",
    "outputId": "ad68be78-f126-44c1-ea76-212ac6b66c41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor_float.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or do it from the beginning when transforming our Numpy array by specifying the ```dtype``` explicitely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytensor_float = torch.tensor(myvector, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor_float.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have the right object to path through our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1610065492214,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "wwrmvWgcYDRU",
    "outputId": "a8e64d64-55d7-46da-8021-45365708dc7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 25.0039,  14.2419, -23.9584,  14.2000, -23.1391, -37.3466,  19.9561,\n",
       "        -10.9655,  31.3603,  46.8313], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = lin_layer(mytensor_float)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1610065494958,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "-6bOnr9bYDRU",
    "outputId": "4f753749-515d-4425-fb9d-e4b4b6186cff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the ouput has as expected as size of 10! Of course we can now add as many other layers as we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer2 = nn.Linear(10,3)\n",
    "output = lin_layer2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation\n",
    "\n",
    "In addition to *layers* we will typically also need activation functions such as soft-max. Those are implemented as modules in ```torch.nn``` as well as functions in ```torch.functional``` which we use here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 4629,
     "status": "ok",
     "timestamp": 1610065427088,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "2-uqNcAGYDRN"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use the functional form, we can pass the output of the above linear layer directly to the activation function, here a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer_activated = F.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.4384, 0.0000, 3.9337], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_layer_activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzPIWGlLYDRN"
   },
   "source": [
    "## Structure of a network\n",
    "\n",
    "Now that we have seen how to create a layers and activation functions we can assemble them into a usable network structure. In PyTorch that structure is ```nn.Module``` a base class on top of which we can build our network. We can specify what parameters we want to pass when creating this object and we also have to define a single function, ```forward```, which describes the network itself. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1610065432065,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "-orgrI3IYDRO"
   },
   "outputs": [],
   "source": [
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(myparameter1, 5)\n",
    "        self.layer2 = nn.Linear(5, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij-rK7HkYDRO"
   },
   "source": [
    "Above, we have defined a simple network that is defined by two parameters, ```myparameter1, myparameter2```, and which is composed of two linear layers and ReLU unit. The differnt layers that we need are defined in ```__init__``` as object parameters and then re-used in the network definition in the ```forward``` function. ```forward``` takes an input ```x``` (e.g. an image to classify), passes it through the network and outputs the result. Therefore in principle we could instantiate a model and use it like this:\n",
    "\n",
    "```\n",
    "mymodel = Mynetwork(9,3)\n",
    "mymodel.forward(myinput)\n",
    "```\n",
    "\n",
    "However, ```nn.Module``` has a ```__call__``` attribute that allows us to use the class as a function like this:\n",
    "\n",
    "```\n",
    "mymodel = Mynetwork(9,3)\n",
    "mymodel(myinput)\n",
    "```\n",
    "\n",
    "This is actually **how a module should be properly used** in order to exploit all capabilities offered in PyTorch.\n",
    "\n",
    "Let's now try this out. We instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1610065434681,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "4nhXAob2YDRO"
   },
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(9,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBL1nCf6YDRP"
   },
   "source": [
    "Just like for the single linear layer before, we can have a look at all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1610065436476,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "HXGXjCKdYDRP",
    "outputId": "cbf73c4a-bbc5-48c8-dadd-ab24d34174f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0916,  0.1267, -0.0580, -0.1453,  0.2853,  0.1041,  0.2018,  0.0017,\n",
       "          -0.2529],\n",
       "         [ 0.0882,  0.0016, -0.2796, -0.3061,  0.1009,  0.0172,  0.1323, -0.0379,\n",
       "           0.1967],\n",
       "         [ 0.1207,  0.0494, -0.2789,  0.2121, -0.3151,  0.1948,  0.0652, -0.1022,\n",
       "          -0.0676],\n",
       "         [-0.0556,  0.2249,  0.3190,  0.1886, -0.2639, -0.3063, -0.1372,  0.2622,\n",
       "          -0.0914],\n",
       "         [-0.1276,  0.2576,  0.0573, -0.2187,  0.1428, -0.2193,  0.0986,  0.1275,\n",
       "           0.2785]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2304, -0.3143, -0.0236, -0.2138,  0.0809], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.3120, -0.2586,  0.2197,  0.4221, -0.0557],\n",
       "         [ 0.3251,  0.4177,  0.0069, -0.4141, -0.0660],\n",
       "         [-0.1065,  0.0632, -0.2560, -0.1078, -0.0625]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0208, -0.0760, -0.1386], requires_grad=True)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vht79uk8YDRQ"
   },
   "source": [
    "The output is the same, only now we see all parameters from all layers, not just a single one. We can also look at the modules contained in our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mynetwork(\n",
       "   (layer1): Linear(in_features=9, out_features=5, bias=True)\n",
       "   (layer2): Linear(in_features=5, out_features=3, bias=True)\n",
       " ),\n",
       " Linear(in_features=9, out_features=5, bias=True),\n",
       " Linear(in_features=5, out_features=3, bias=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some repeats because we see here modules at all levels. E.g. each linear layer is a module but our entire network is a module as well.\n",
    "\n",
    "We can also just find all modules contained in our main module and recover its name and function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1610065440522,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "kp8wjdEPYDRQ",
    "outputId": "b1c92855-78e6-450f-bcca-28dd9d3ccd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: layer1 module: Linear(in_features=9, out_features=5, bias=True)\n",
      "name: layer2 module: Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in mymodel.named_children():\n",
    "    print(f'name: {name} module: {module}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmhrocc9YDRR"
   },
   "source": [
    "We can also obtain a dictionary of all layers with their weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1610065444665,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "1MrDaJ4hYDRR",
    "outputId": "20c0d36d-d842-4fed-bcee-973ac6316436"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.0916,  0.1267, -0.0580, -0.1453,  0.2853,  0.1041,  0.2018,  0.0017,\n",
       "                       -0.2529],\n",
       "                      [ 0.0882,  0.0016, -0.2796, -0.3061,  0.1009,  0.0172,  0.1323, -0.0379,\n",
       "                        0.1967],\n",
       "                      [ 0.1207,  0.0494, -0.2789,  0.2121, -0.3151,  0.1948,  0.0652, -0.1022,\n",
       "                       -0.0676],\n",
       "                      [-0.0556,  0.2249,  0.3190,  0.1886, -0.2639, -0.3063, -0.1372,  0.2622,\n",
       "                       -0.0914],\n",
       "                      [-0.1276,  0.2576,  0.0573, -0.2187,  0.1428, -0.2193,  0.0986,  0.1275,\n",
       "                        0.2785]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.2304, -0.3143, -0.0236, -0.2138,  0.0809])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.3120, -0.2586,  0.2197,  0.4221, -0.0557],\n",
       "                      [ 0.3251,  0.4177,  0.0069, -0.4141, -0.0660],\n",
       "                      [-0.1065,  0.0632, -0.2560, -0.1078, -0.0625]])),\n",
       "             ('layer2.bias', tensor([ 0.0208, -0.0760, -0.1386]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can pass an input through our network. It takes an input of size 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = torch.randn((9,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mymodel(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2689, -0.1432, -0.3604], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKI4bj_YYDRU"
   },
   "source": [
    "### Batches\n",
    "\n",
    "If we want to use batch processing, we can also pass batches of vectors thourgh the network instead of single vectors. PyTroch layers are designed to hanle this by default, so here for example if we want to use a batch of size 32 we can just use a 32 x 8 tensor. We create one directly with ```torch.randn``` here (we will learn more about batches in the chapter on training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1610065511425,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "b2FsgyCfYDRV",
    "outputId": "ae1ae915-54bd-402a-d963-eef0052600f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor = torch.randn(32,9)\n",
    "batch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1610065512351,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "bYAmmXLwYDRV"
   },
   "outputs": [],
   "source": [
    "batch_output = mymodel(batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1610065516126,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "UP48jBj2YDRV",
    "outputId": "8adf830c-c1de-465f-c53f-b7c3b7fbeea7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output is \"batched\" as well, so the network gracefully handlend this batch for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm35UQ76YDRV"
   },
   "source": [
    "### Passing an image as input\n",
    "\n",
    "For the tests above, we simply used vectors. However our goal is to pass images to our network. As our network is linear and fully connected, we therefore need to pass a flattened (1D) version of our image. We have seen before that we can do that using the ```view``` method or ```flatten```.\n",
    "\n",
    "To test this, we first create a test image using the ```skimage.draw``` utility scikit-image which allows us to create ```random_shapes``` images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1610065523234,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "oWDgsh30YDRV"
   },
   "outputs": [],
   "source": [
    "from skimage.draw import random_shapes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a 32x32 image of a circle. We have multiple options for the number of shapes, their size etc. to select from. We also invert the image (to have the *object* as foreground):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1610065523543,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "fJrw_f5SYDRV"
   },
   "outputs": [],
   "source": [
    "image_circle, _ = random_shapes((32,32),max_shapes=1, min_shapes=1, shape='circle',\n",
    "                                num_channels=1, min_size=8, random_seed=2)\n",
    "image_circle = 255-image_circle\n",
    "\n",
    "image_tensor = torch.tensor(image_circle,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1610065524861,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "4EFYRWMBYDRW",
    "outputId": "7edbbcb8-79e2-4161-ff2f-0b82f457290b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we flatten the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1610065525997,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "tK7em6_XYDRW",
    "outputId": "979aea7b-6159-44bd-f0d5-4721056cffc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_flat = image_tensor.view(-1)\n",
    "image_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugGq_DqGYDRW"
   },
   "source": [
    "We adjust our network input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1610065527813,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "YKDedfAmYDRW"
   },
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(1024,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1610065528756,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "2nHvFUbUYDRW"
   },
   "outputs": [],
   "source": [
    "output = mymodel(image_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1610065532219,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "B4I8aSoSYDRX",
    "outputId": "b7a50671-246e-4151-c8d8-85b0b30fc788"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1637,  0.4857, -0.5744,  0.9094,  1.2106,  0.5750, -0.4645,  0.7703,\n",
       "         0.6102, -0.3215,  0.9214,  0.1671, -0.9559, -0.2735,  0.5314,  0.2064,\n",
       "        -1.1113,  1.3897,  0.4575,  0.0441], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjTKDBKJYDRX"
   },
   "source": [
    "The operation of \"flattening\" can be done at different places. For example, instead of flattening the input, we can also flatten within our network definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1610065538567,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "Xg_6FfmmYDRX"
   },
   "outputs": [],
   "source": [
    "class Mynetwork2(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork2, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(myparameter1, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1)\n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1610065539912,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "FVwBTjLfYDRX"
   },
   "outputs": [],
   "source": [
    "second_model = Mynetwork2(1024, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1610065542870,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "1de0QvgnYDRY",
    "outputId": "5dd4db98-d8ce-4dd2-db31-fe525d407371"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2187, 0.0000, 0.0000, 0.0000, 0.0825, 8.2140, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 5.7136, 0.0000, 3.0736, 0.0000, 1.5354, 0.5345, 0.0000,\n",
       "        0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = second_model(image_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1610065544375,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "LB7DUByhYDRY",
    "outputId": "56dc6213-3dba-4997-c1f2-53d0e5e28b94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87EQqckMYDRY"
   },
   "source": [
    "## Saving and loading a model\n",
    "\n",
    "Our next goal will be to train our network (see next chapter). At some point during or after training we will want to save both our model and its weights. This can be done in two ways.\n",
    "\n",
    "### Save full model\n",
    "\n",
    "First, we can save the entire model so that it can be simply reloaded. This is practical when developing but can lead to problems when moving saved models between computers. First we save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "OYkt0Fw0YDRY"
   },
   "outputs": [],
   "source": [
    "torch.save(second_model, datapath.joinpath('models/simpleNN.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVR_snhCYDRZ"
   },
   "source": [
    "And reload it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "e4sjsS7KYDRZ"
   },
   "outputs": [],
   "source": [
    "third_model = torch.load(datapath.joinpath('models/simpleNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B5BcgpK9YDRZ",
    "outputId": "d48bb150-16f2-4706-afdb-7784f05b779f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2187, 0.0000, 0.0000, 0.0000, 0.0825, 8.2140, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 5.7136, 0.0000, 3.0736, 0.0000, 1.5354, 0.5345, 0.0000,\n",
       "        0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlCrSWAFYDRZ"
   },
   "source": [
    "And we see that we obtain the same values as above, meaning that the parameters were indeed saved.\n",
    "\n",
    "### Saving the model state\n",
    "\n",
    "Alternatively we can simply save all the parameters, which is a safer method. We simply recover them using ```state_dict```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "OoBQM5drYDRZ"
   },
   "outputs": [],
   "source": [
    "torch.save(third_model.state_dict(),datapath.joinpath('models/simpleNN_state.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WORns8K-YDRZ"
   },
   "source": [
    "To reload the parameters we of course now have to first instantiate the model and \"fill\" it with those values using ```load_state_dict```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "wGZ_UAqhYDRZ"
   },
   "outputs": [],
   "source": [
    "fourth_model = Mynetwork2(1024, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "WxQoeTofYDRa",
    "outputId": "f023dd3b-e0a0-45c8-e688-5592b0385528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.0207, -0.0005,  0.0177,  ...,  0.0241,  0.0137,  0.0059],\n",
       "                      [-0.0034,  0.0095, -0.0191,  ...,  0.0249,  0.0082,  0.0123],\n",
       "                      [ 0.0116,  0.0266,  0.0159,  ...,  0.0054, -0.0265,  0.0298],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0254,  0.0149,  ..., -0.0304, -0.0112, -0.0013],\n",
       "                      [-0.0154,  0.0142, -0.0148,  ..., -0.0086,  0.0168, -0.0268],\n",
       "                      [ 0.0259, -0.0178,  0.0232,  ..., -0.0163, -0.0196,  0.0164]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.0153, -0.0287,  0.0084, -0.0152,  0.0101, -0.0208, -0.0010,  0.0028,\n",
       "                      -0.0006,  0.0201, -0.0198,  0.0070, -0.0310,  0.0186, -0.0018,  0.0201,\n",
       "                       0.0171, -0.0085,  0.0082,  0.0292]))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load(datapath.joinpath('models/simpleNN_state.pt'))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OEBX9c-nYDRa",
    "outputId": "b7dfbf2e-64b4-4c13-d91d-1ad80b6e2195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "z9oYFeQhYDRa",
    "outputId": "e820c7b9-a993-49dd-be0e-9ae7b559cd39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2187, 0.0000, 0.0000, 0.0000, 0.0825, 8.2140, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 5.7136, 0.0000, 3.0736, 0.0000, 1.5354, 0.5345, 0.0000,\n",
       "        0.0000, 0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x3iMsO-YDRa"
   },
   "source": [
    "## Running on a GPU (run on Colab)\n",
    "\n",
    "If you want to exploit the computational power of a GPU you have to take care of moving **both** the model and the data to it. First, find out if you have a GPU. You can test this for example by running this notebook on Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1610065588350,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "GmDZf0pBYDRa",
    "outputId": "d3cc82a5-e09a-4b25-b731-efae63e890e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxxUBfwOYDRb"
   },
   "source": [
    "If you have a GPU, you can define it as a device. If not you can fall back on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1610065594926,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "A-L5ErcoYDRb"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_arWP1_YDRb"
   },
   "source": [
    "Now finally if you want to use it with a model you need to move the model to it. Let's check where it currently sits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10919,
     "status": "ok",
     "timestamp": 1610066047097,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "V0wwfn2lY2am",
    "outputId": "f71ec68f-d846-4198-a25d-013b5da6980b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mynetwork2(\n",
       "  (layer1): Linear(in_features=1024, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1610066136339,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "vA7pOTxXawjR",
    "outputId": "9921d8f3-1060-4f0f-8c8d-1d34dd839ff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0006,  0.0285, -0.0012,  ..., -0.0120, -0.0017,  0.0108],\n",
       "         [-0.0153, -0.0075, -0.0311,  ..., -0.0122, -0.0052, -0.0173],\n",
       "         [ 0.0219,  0.0307,  0.0298,  ..., -0.0292, -0.0145, -0.0102],\n",
       "         ...,\n",
       "         [ 0.0202, -0.0083, -0.0123,  ..., -0.0279,  0.0008, -0.0194],\n",
       "         [-0.0104, -0.0002,  0.0236,  ...,  0.0193, -0.0260,  0.0115],\n",
       "         [-0.0140,  0.0290,  0.0227,  ..., -0.0297,  0.0002, -0.0003]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0247, -0.0022, -0.0153,  0.0309, -0.0280, -0.0003, -0.0287,  0.0284,\n",
       "         -0.0312,  0.0260, -0.0283, -0.0141, -0.0034, -0.0200, -0.0104, -0.0007,\n",
       "          0.0278,  0.0278, -0.0049,  0.0208], requires_grad=True)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(second_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-_sgXVsbPOU"
   },
   "source": [
    "Now if we try to pass our current image through the network we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "error",
     "timestamp": 1610066264714,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "FAqQgKribVaX",
    "outputId": "8c357504-908e-4369-8354-a1c588583af7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58.9915,  0.0000,  0.0000,  0.0000,  0.0000, 30.0834,  0.0000, 30.6602,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, 27.1779,  0.0000, 44.7265,  0.0000,\n",
       "        13.0921, 47.9660,  0.0000,  0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM379mcZYDRc"
   },
   "source": [
    "The error message is quite explicit: you then need also to move the data, both for training and inference to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1610066318163,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "prB7rQBoYDRc",
    "outputId": "78b1a0a8-0e5b-4813-91aa-95641a983b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1610066328705,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "sGgYqbWpYDRc"
   },
   "outputs": [],
   "source": [
    "image_tensor = image_tensor.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1610066369153,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "_50Ib-_xYDRc"
   },
   "outputs": [],
   "source": [
    "output = second_model(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1610066373868,
     "user": {
      "displayName": "Guillaume Witz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgT0K2JVYzEsjzsS5nhkUVjUrSIJ5jHzXnBoYrmVf8=s64",
      "userId": "16033870147214403532"
     },
     "user_tz": -60
    },
    "id": "dDX-VKv9bqrj",
    "outputId": "2e45ed96-3def-4bf7-bf71-482fedc87432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58.9915,  0.0000,  0.0000,  0.0000,  0.0000, 30.0834,  0.0000, 30.6602,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, 27.1779,  0.0000, 44.7265,  0.0000,\n",
       "        13.0921, 47.9660,  0.0000,  0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oor9GgLbcKXX"
   },
   "source": [
    "The output sits on the GPU and is part of the gradient calcualtion, so if you want to further use it, you will have to pull it from the GPU **and** detach it from gradient calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "pY-wZED3cNBd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58.991543,  0.      ,  0.      ,  0.      ,  0.      , 30.083395,\n",
       "        0.      , 30.660238,  0.      ,  0.      ,  0.      ,  0.      ,\n",
       "       27.177948,  0.      , 44.726513,  0.      , 13.092113, 47.96598 ,\n",
       "        0.      ,  0.      ], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a network with 4 successive linear layers of size 20,40 and 100 and 2, and ReLU activation after the three first layers. It takes a 2D input.\n",
    "2. With ```skimage.io``` import the image in ```.../data/woody_baille.JPG```\n",
    "3. Instantiate your model with the appropriate size\n",
    "4. Run the image through the network and turn the output into a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hlCrSWAFYDRZ"
   ],
   "name": "Simple_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
