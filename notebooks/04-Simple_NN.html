
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Simple neural net with PyTorch &#8212; Deep Learning for imaging</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Training a network" href="05-Training.html" />
    <link rel="prev" title="3. Tensor calculations" href="03-Tensors.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for imaging</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Readme.html">
   Deep Learning for imaging
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Basics_image_processing.html">
   1. Basics of image handling and processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-Simple_inference_vgg16.html">
   2. Very accessible deep learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Tensors.html">
   3. Tensor calculations
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Simple neural net with PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-Training.html">
   5. Training a network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Data_handling.html">
   6. Handling data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Augmentation.html">
   7. Data augmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Lightning.html">
   8. Simplifying code with PyTorch-Lightning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Classify_drawings.html">
   9. Classification: practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Convolutions_draw.html">
   10. Convolutions and rescaling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11-Autoencoder_drawings.html">
   11. Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12-TransferLearning_worms.html">
   12. Transfer learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-Segmentation.html">
   13. Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Unet.html">
   14. U-net
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-Unet_nuclei.html">
   15. Unet applied to nuclei segmentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/04-Simple_NN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/04-Simple_NN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-from-torch-nn">
   Layer from torch.nn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#passing-an-input">
   Passing an input
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation">
   Activation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structure-of-a-network">
   Structure of a network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batches">
     Batches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#passing-an-image-as-input">
     Passing an image as input
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-and-loading-a-model">
   Saving and loading a model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-full-model">
     Save full model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-the-model-state">
     Saving the model state
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-on-a-gpu-run-on-colab">
   Running on a GPU (run on Colab)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4. Simple neural net with PyTorch</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-from-torch-nn">
   Layer from torch.nn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#passing-an-input">
   Passing an input
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation">
   Activation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#structure-of-a-network">
   Structure of a network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batches">
     Batches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#passing-an-image-as-input">
     Passing an image as input
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-and-loading-a-model">
   Saving and loading a model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-full-model">
     Save full model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#saving-the-model-state">
     Saving the model state
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-on-a-gpu-run-on-colab">
   Running on a GPU (run on Colab)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a class="reference external" href="https://colab.research.google.com/github/guiwitz/DLImaging/blob/master/notebooks/04-Simple_NN.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="simple-neural-net-with-pytorch">
<h1>4. Simple neural net with PyTorch<a class="headerlink" href="#simple-neural-net-with-pytorch" title="Permalink to this headline">¶</a></h1>
<p>Neural networks can be programmed on different levels depending on how much one needs to customize either the architecture or the training pattern. When dealing with more complex NN we will use a higher-level package (Lightning, see <a class="reference internal" href="08-Lightning.html"><span class="doc std std-doc">Chapter 8</span></a>) which will spare us some “manual” work. However, in order to understand all steps involved in designing a DL algorithm, we will first use basic PyTroch. In this notebook we will see howe we can <em>design</em> a NN while in <a class="reference internal" href="05-Training.html"><span class="doc std std-doc">the next notebook</span></a> we will se how to train it.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set path containing data folder or use default for Colab (/gdrive/My Drive)</span>
<span class="n">local_folder</span> <span class="o">=</span> <span class="s2">&quot;../&quot;</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py&#39;</span><span class="p">,</span> <span class="s1">&#39;check_colab.py&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">check_colab</span> <span class="kn">import</span> <span class="n">set_datapath</span>
<span class="n">colab</span><span class="p">,</span> <span class="n">datapath</span> <span class="o">=</span> <span class="n">set_datapath</span><span class="p">(</span><span class="n">local_folder</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mounted at /gdrive
</pre></div>
</div>
</div>
</div>
<div class="section" id="layer-from-torch-nn">
<h2>Layer from torch.nn<a class="headerlink" href="#layer-from-torch-nn" title="Permalink to this headline">¶</a></h2>
<p>We will see in this notebook how to create a simple neural network for image classification using the most general mechanism in PyTorch which are modules. All module-related objects and functions are contained in <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>. As the name indicates, these modules are really <em>modular</em> in the sense that they are indpendent parts of networks that can be combined together. Mostly we will use single modules containing the multiple layers of our architecture, but we will also see examples where multiple modules are assembled.</p>
<p>A module is essentially an assemble of multiple other modules, and many of these are provided “pre-made” for us also available in <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>. For example in the simple network used here we will only use linear layers. Before we create an actual network let’s see how layers work:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
</div>
</div>
<p>We creat now a linear layer that takes a vector of size 5 as input and ouptuts a vector of size 10:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This single layer already has parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">lin_layer</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[ 0.1772, -0.2716, -0.0833,  0.4179,  0.0139],
         [ 0.0236,  0.4233, -0.0906, -0.0667, -0.1381],
         [ 0.0183,  0.3365, -0.0383, -0.2230,  0.1193],
         [-0.1372, -0.1642,  0.0699,  0.2822, -0.0746],
         [-0.4264,  0.3427, -0.1457, -0.4305, -0.0769],
         [-0.1531, -0.0913,  0.1122, -0.1215, -0.0599],
         [-0.0187,  0.1361, -0.3873,  0.2272, -0.3454],
         [-0.3957, -0.3610, -0.0309,  0.3582,  0.2156],
         [ 0.3501,  0.2927, -0.1047,  0.4398, -0.1033],
         [ 0.2378,  0.0654, -0.3990, -0.2655,  0.1689]], requires_grad=True),
 Parameter containing:
 tensor([-0.2049, -0.0815,  0.4178, -0.3250,  0.3191, -0.3232, -0.1012,  0.0951,
          0.0309, -0.1550], requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<p>We see that we have indeed a 5x10 matrix plus the bias term of size 10. When we later assemble multiple modules, we will see the same time of output but with many more parameters.</p>
</div>
<div class="section" id="passing-an-input">
<h2>Passing an input<a class="headerlink" href="#passing-an-input" title="Permalink to this headline">¶</a></h2>
<p>Our layer takes a vector as an input so let’s try to creat a vector of size 5 and pass it through the layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">myvector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">myvector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([93, 93, 44, 15, 90])
</pre></div>
</div>
</div>
</div>
<p>No we we pass it to <code class="docutils literal notranslate"><span class="pre">lin_layer</span></code>:</p>
<div class="cell tag_raises-exception docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer</span><span class="p">(</span><span class="n">myvector</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-7-6ef7c06b0ae8&gt;</span> in <span class="ni">&lt;module&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">lin_layer</span><span class="p">(</span><span class="n">myvector</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> 
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">103</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span> 
<span class="g g-Whitespace">    </span><span class="mi">105</span>     <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py</span> in <span class="ni">linear</span><span class="nt">(input, weight, bias)</span>
<span class="g g-Whitespace">   </span><span class="mi">1846</span>     <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1847</span>         <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1848</span>     <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1849</span> 
<span class="g g-Whitespace">   </span><span class="mi">1850</span> 

<span class="ne">TypeError</span>: linear(): argument &#39;input&#39; (position 1) must be Tensor, not numpy.ndarray
</pre></div>
</div>
</div>
</div>
<p>Of course we get an error, as the layer doesn’t expect a Numpy array but a PyTorch tensor! We can have a look at what PyTorch tensors are in the <a class="reference internal" href="03-Tensors.html"><span class="doc std std-doc">03-Tensors</span></a> notebook and come back here later.</p>
<p>Now we can turn our Numpy array into a tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">myvector</span><span class="p">)</span>
<span class="n">mytensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([93, 93, 44, 15, 90])
</pre></div>
</div>
</div>
</div>
<div class="cell tag_raises-exception docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer</span><span class="p">(</span><span class="n">mytensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-10-4eeffe384bb6&gt;</span> in <span class="ni">&lt;module&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">lin_layer</span><span class="p">(</span><span class="n">mytensor</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> 
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">103</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span> 
<span class="g g-Whitespace">    </span><span class="mi">105</span>     <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py</span> in <span class="ni">linear</span><span class="nt">(input, weight, bias)</span>
<span class="g g-Whitespace">   </span><span class="mi">1846</span>     <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1847</span>         <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1848</span>     <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1849</span> 
<span class="g g-Whitespace">   </span><span class="mi">1850</span> 

<span class="ne">RuntimeError</span>: expected scalar type Long but found Float
</pre></div>
</div>
</div>
</div>
<p>Again an error! The error can be a bit misleading. The whole problem is that the weights in the layer are defined by default as <code class="docutils literal notranslate"><span class="pre">float32</span></code>. Hence the input should match this but we passed an 64 bits integer which creates a conflict. Let’s adjust the type of our tensor. We can either modify the tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_float</span> <span class="o">=</span> <span class="n">mytensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_float</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([93., 93., 44., 15., 90.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_float</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.float32
</pre></div>
</div>
</div>
</div>
<p>or do it from the beginning when transforming our Numpy array by specifying the <code class="docutils literal notranslate"><span class="pre">dtype</span></code> explicitely:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_float</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">myvector</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_float</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.float32
</pre></div>
</div>
</div>
</div>
<p>Now we finally have the right object to path through our network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">lin_layer</span><span class="p">(</span><span class="n">mytensor_float</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ -5.1316,  24.0661,  39.1124, -27.7620, -27.2561, -25.3290, -33.9091,
        -46.8651,  52.5092,  21.6992], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([10])
</pre></div>
</div>
</div>
</div>
<p>We see that the ouput has as expected as size of 10! Of course we can now add as many other layers as we want:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">lin_layer2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([3])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="activation">
<h2>Activation<a class="headerlink" href="#activation" title="Permalink to this headline">¶</a></h2>
<p>In addition to <em>layers</em> we will typically also need activation functions such as soft-max. Those are implemented as modules in <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> as well as functions in <code class="docutils literal notranslate"><span class="pre">torch.functional</span></code> which we use here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
</div>
<p>Since we use the functional form, we can pass the output of the above linear layer directly to the activation function, here a ReLU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer_activated</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin_layer_activated</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.6315, 0.0000, 0.0000], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="structure-of-a-network">
<h2>Structure of a network<a class="headerlink" href="#structure-of-a-network" title="Permalink to this headline">¶</a></h2>
<p>Now that we have seen how to create a layers and activation functions we can assemble them into a usable network structure. In PyTorch that structure is <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> a base class on top of which we can build our network. We can specify what parameters we want to pass when creating this object and we also have to define a single function, <code class="docutils literal notranslate"><span class="pre">forward</span></code>, which describes the network itself. Here’s an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Mynetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">myparameter1</span><span class="p">,</span> <span class="n">myparameter2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mynetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># define e.g. layers here e.g.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">myparameter1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">myparameter2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="c1"># define the sequence of operations in the network including e.g. activations</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Above, we have defined a simple network that is defined by two parameters, <code class="docutils literal notranslate"><span class="pre">myparameter1,</span> <span class="pre">myparameter2</span></code>, and which is composed of two linear layers and ReLU unit. The differnt layers that we need are defined in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> as object parameters and then re-used in the network definition in the <code class="docutils literal notranslate"><span class="pre">forward</span></code> function. <code class="docutils literal notranslate"><span class="pre">forward</span></code> takes an input <code class="docutils literal notranslate"><span class="pre">x</span></code> (e.g. an image to classify), passes it through the network and outputs the result. Therefore in principle we could instantiate a model and use it like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mymodel</span> <span class="o">=</span> <span class="n">Mynetwork</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">mymodel</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">myinput</span><span class="p">)</span>
</pre></div>
</div>
<p>However, <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> has a <code class="docutils literal notranslate"><span class="pre">__call__</span></code> attribute that allows us to use the class as a function like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mymodel</span> <span class="o">=</span> <span class="n">Mynetwork</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">mymodel</span><span class="p">(</span><span class="n">myinput</span><span class="p">)</span>
</pre></div>
</div>
<p>This is actually <strong>how a module should be properly used</strong> in order to exploit all capabilities offered in PyTorch.</p>
<p>Let’s now try this out. We instantiate the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mymodel</span> <span class="o">=</span> <span class="n">Mynetwork</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just like for the single linear layer before, we can have a look at all parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">mymodel</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[ 0.0483, -0.2518, -0.0089, -0.2222, -0.2226,  0.0678, -0.0117,  0.1295,
          -0.2652],
         [ 0.1032,  0.1222,  0.1620, -0.3203,  0.1332,  0.0745, -0.2601,  0.1932,
           0.0618],
         [ 0.3046,  0.0502,  0.1105,  0.0564, -0.1664, -0.0811,  0.1039,  0.0440,
           0.1922],
         [ 0.0937,  0.2563,  0.1606,  0.1038, -0.0500, -0.2383,  0.1778, -0.0871,
           0.1092],
         [-0.3318, -0.2407, -0.0272,  0.2387, -0.2174,  0.1162, -0.1696,  0.2630,
          -0.3218]], requires_grad=True), Parameter containing:
 tensor([ 0.1442,  0.3127, -0.3283,  0.2729, -0.2035], requires_grad=True), Parameter containing:
 tensor([[-0.3275, -0.3544, -0.0413,  0.1119, -0.1094],
         [-0.3663,  0.0042, -0.4315, -0.1468,  0.0556],
         [ 0.3742, -0.1017,  0.3442,  0.3889, -0.3229]], requires_grad=True), Parameter containing:
 tensor([ 0.1876,  0.3002, -0.2017], requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<p>The output is the same, only now we see all parameters from all layers, not just a single one. We can also look at the modules contained in our network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">mymodel</span><span class="o">.</span><span class="n">modules</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Mynetwork(
   (layer1): Linear(in_features=9, out_features=5, bias=True)
   (layer2): Linear(in_features=5, out_features=3, bias=True)
 ),
 Linear(in_features=9, out_features=5, bias=True),
 Linear(in_features=5, out_features=3, bias=True)]
</pre></div>
</div>
</div>
</div>
<p>We see some repeats because we see here modules at all levels. E.g. each linear layer is a module but our entire network is a module as well.</p>
<p>We can also just find all modules contained in our main module and recover its name and function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">mymodel</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;name: </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1"> module: </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>name: layer1 module: Linear(in_features=9, out_features=5, bias=True)
name: layer2 module: Linear(in_features=5, out_features=3, bias=True)
</pre></div>
</div>
</div>
</div>
<p>We can also obtain a dictionary of all layers with their weights:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mymodel</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;layer1.weight&#39;,
              tensor([[ 0.0483, -0.2518, -0.0089, -0.2222, -0.2226,  0.0678, -0.0117,  0.1295,
                       -0.2652],
                      [ 0.1032,  0.1222,  0.1620, -0.3203,  0.1332,  0.0745, -0.2601,  0.1932,
                        0.0618],
                      [ 0.3046,  0.0502,  0.1105,  0.0564, -0.1664, -0.0811,  0.1039,  0.0440,
                        0.1922],
                      [ 0.0937,  0.2563,  0.1606,  0.1038, -0.0500, -0.2383,  0.1778, -0.0871,
                        0.1092],
                      [-0.3318, -0.2407, -0.0272,  0.2387, -0.2174,  0.1162, -0.1696,  0.2630,
                       -0.3218]])),
             (&#39;layer1.bias&#39;,
              tensor([ 0.1442,  0.3127, -0.3283,  0.2729, -0.2035])),
             (&#39;layer2.weight&#39;,
              tensor([[-0.3275, -0.3544, -0.0413,  0.1119, -0.1094],
                      [-0.3663,  0.0042, -0.4315, -0.1468,  0.0556],
                      [ 0.3742, -0.1017,  0.3442,  0.3889, -0.3229]])),
             (&#39;layer2.bias&#39;, tensor([ 0.1876,  0.3002, -0.2017]))])
</pre></div>
</div>
</div>
</div>
<p>Finally we can pass an input through our network. It takes an input of size 9:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">my_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">9</span><span class="p">,))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">my_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([-0.3433,  0.0139,  0.0470], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="batches">
<h3>Batches<a class="headerlink" href="#batches" title="Permalink to this headline">¶</a></h3>
<p>If we want to use batch processing, we can also pass batches of vectors thourgh the network instead of single vectors. PyTroch layers are designed to hanle this by default, so here for example if we want to use a batch of size 32 we can just use a 32 x 8 tensor. We create one directly with <code class="docutils literal notranslate"><span class="pre">torch.randn</span></code> here (we will learn more about batches in the chapter on training):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>
<span class="n">batch_tensor</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 9])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_output</span> <span class="o">=</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">batch_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 3])
</pre></div>
</div>
</div>
</div>
<p>We see that the output is “batched” as well, so the network gracefully handlend this batch for us.</p>
</div>
<div class="section" id="passing-an-image-as-input">
<h3>Passing an image as input<a class="headerlink" href="#passing-an-image-as-input" title="Permalink to this headline">¶</a></h3>
<p>For the tests above, we simply used vectors. However our goal is to pass images to our network. As our network is linear and fully connected, we therefore need to pass a flattened (1D) version of our image. We have seen before that we can do that using the <code class="docutils literal notranslate"><span class="pre">view</span></code> method or <code class="docutils literal notranslate"><span class="pre">flatten</span></code>.</p>
<p>To test this, we first create a test image using the <code class="docutils literal notranslate"><span class="pre">skimage.draw</span></code> utility scikit-image which allows us to create <code class="docutils literal notranslate"><span class="pre">random_shapes</span></code> images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skimage.draw</span> <span class="kn">import</span> <span class="n">random_shapes</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>We create a 32x32 image of a circle. We have multiple options for the number of shapes, their size etc. to select from. We also invert the image (to have the <em>object</em> as foreground):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_circle</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">random_shapes</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span><span class="n">max_shapes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_shapes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s1">&#39;circle&#39;</span><span class="p">,</span>
                                <span class="n">num_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">image_circle</span> <span class="o">=</span> <span class="mi">255</span><span class="o">-</span><span class="n">image_circle</span>

<span class="n">image_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">image_circle</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_tensor</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([32, 32])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04-Simple_NN_62_0.png" src="../_images/04-Simple_NN_62_0.png" />
</div>
</div>
<p>Now we flatten the image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_flat</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">image_flat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1024])
</pre></div>
</div>
</div>
</div>
<p>We adjust our network input size:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mymodel</span> <span class="o">=</span> <span class="n">Mynetwork</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">mymodel</span><span class="p">(</span><span class="n">image_flat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.3483,  2.5287, -0.9607, -0.6729,  0.8906,  0.7305,  2.4549, -1.1527,
         1.9908, -1.9030, -1.1623, -0.4935,  1.7990,  1.3409, -0.3435, -1.2225,
        -1.7168,  2.4262,  0.0932, -1.4792], grad_fn=&lt;AddBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>The operation of “flattening” can be done at different places. For example, instead of flattening the input, we can also flatten within our network definition:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Mynetwork2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">myparameter1</span><span class="p">,</span> <span class="n">myparameter2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mynetwork2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># define e.g. layers here e.g.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">myparameter1</span><span class="p">,</span> <span class="n">myparameter2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># define the sequence of operations in the network including e.g. activations</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">second_model</span> <span class="o">=</span> <span class="n">Mynetwork2</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">second_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
<span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2747, 12.7591,  0.0000,  0.0000,
         3.2894,  3.1473,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        14.3413,  7.2331,  0.0000,  0.0000], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([20])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="saving-and-loading-a-model">
<h2>Saving and loading a model<a class="headerlink" href="#saving-and-loading-a-model" title="Permalink to this headline">¶</a></h2>
<p>Our next goal will be to train our network (see next chapter). At some point during or after training we will want to save both our model and its weights. This can be done in two ways.</p>
<div class="section" id="save-full-model">
<h3>Save full model<a class="headerlink" href="#save-full-model" title="Permalink to this headline">¶</a></h3>
<p>First, we can save the entire model so that it can be simply reloaded. This is practical when developing but can lead to problems when moving saved models between computers. First we save our model (create a <code class="docutils literal notranslate"><span class="pre">models</span></code> folder in your directory if needed):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">second_model</span><span class="p">,</span> <span class="n">datapath</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s1">&#39;models/simpleNN.pt&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>And reload it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">third_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">datapath</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s1">&#39;models/simpleNN.pt&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">third_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2747, 12.7591,  0.0000,  0.0000,
         3.2894,  3.1473,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        14.3413,  7.2331,  0.0000,  0.0000], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>And we see that we obtain the same values as above, meaning that the parameters were indeed saved.</p>
</div>
<div class="section" id="saving-the-model-state">
<h3>Saving the model state<a class="headerlink" href="#saving-the-model-state" title="Permalink to this headline">¶</a></h3>
<p>Alternatively we can simply save all the parameters, which is a safer method. We simply recover them using <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">third_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="n">datapath</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s1">&#39;models/simpleNN_state.pt&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>To reload the parameters we of course now have to first instantiate the model and “fill” it with those values using <code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fourth_model</span> <span class="o">=</span> <span class="n">Mynetwork2</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">datapath</span><span class="o">.</span><span class="n">joinpath</span><span class="p">(</span><span class="s1">&#39;models/simpleNN_state.pt&#39;</span><span class="p">))</span>
<span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>OrderedDict([(&#39;layer1.weight&#39;,
              tensor([[-6.5818e-03, -1.3050e-05, -2.1981e-02,  ...,  2.0436e-02,
                       -1.8892e-03, -2.1725e-02],
                      [ 2.0734e-02, -1.9473e-02, -2.8260e-02,  ...,  2.5199e-02,
                       -2.9951e-02,  8.9792e-03],
                      [-1.3109e-03,  1.3450e-02,  1.9928e-02,  ..., -5.7713e-03,
                        1.0590e-02, -1.9134e-02],
                      ...,
                      [ 1.6805e-02,  2.3167e-02,  3.1948e-03,  ..., -1.5454e-02,
                        1.3553e-02, -1.7430e-02],
                      [-2.3587e-02, -1.8857e-02,  9.4816e-03,  ...,  2.1235e-02,
                       -2.0854e-02, -2.8723e-02],
                      [ 2.1510e-03, -4.4486e-03, -1.7398e-02,  ..., -1.2099e-02,
                        1.0446e-02,  2.7855e-03]])),
             (&#39;layer1.bias&#39;,
              tensor([ 0.0084,  0.0012,  0.0020,  0.0185,  0.0250, -0.0038,  0.0236, -0.0299,
                       0.0036,  0.0277, -0.0210, -0.0126, -0.0142, -0.0256, -0.0099,  0.0236,
                       0.0120, -0.0239,  0.0216, -0.0094]))])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fourth_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fourth_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2747, 12.7591,  0.0000,  0.0000,
         3.2894,  3.1473,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        14.3413,  7.2331,  0.0000,  0.0000], grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="running-on-a-gpu-run-on-colab">
<h2>Running on a GPU (run on Colab)<a class="headerlink" href="#running-on-a-gpu-run-on-colab" title="Permalink to this headline">¶</a></h2>
<p>If you want to exploit the computational power of a GPU you have to take care of moving <strong>both</strong> the model and the data to it. First, find out if you have a GPU. You can test this for example by running this notebook on Colab:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>If you have a GPU, you can define it as a device. If not you can fall back on CPU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now finally if you want to use it with a model you need to move the model to it. Let’s check where it currently sits:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">second_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mynetwork2(
  (layer1): Linear(in_features=1024, out_features=20, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">second_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Parameter containing:
 tensor([[-6.5818e-03, -1.3050e-05, -2.1981e-02,  ...,  2.0436e-02,
          -1.8892e-03, -2.1725e-02],
         [ 2.0734e-02, -1.9473e-02, -2.8260e-02,  ...,  2.5199e-02,
          -2.9951e-02,  8.9792e-03],
         [-1.3109e-03,  1.3450e-02,  1.9928e-02,  ..., -5.7713e-03,
           1.0590e-02, -1.9134e-02],
         ...,
         [ 1.6805e-02,  2.3167e-02,  3.1948e-03,  ..., -1.5454e-02,
           1.3553e-02, -1.7430e-02],
         [-2.3587e-02, -1.8857e-02,  9.4816e-03,  ...,  2.1235e-02,
          -2.0854e-02, -2.8723e-02],
         [ 2.1510e-03, -4.4486e-03, -1.7398e-02,  ..., -1.2099e-02,
           1.0446e-02,  2.7855e-03]], device=&#39;cuda:0&#39;, requires_grad=True),
 Parameter containing:
 tensor([ 0.0084,  0.0012,  0.0020,  0.0185,  0.0250, -0.0038,  0.0236, -0.0299,
          0.0036,  0.0277, -0.0210, -0.0126, -0.0142, -0.0256, -0.0099,  0.0236,
          0.0120, -0.0239,  0.0216, -0.0094], device=&#39;cuda:0&#39;,
        requires_grad=True)]
</pre></div>
</div>
</div>
</div>
<p>Now if we try to pass our current image through the network we get an error:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">second_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-62-3dd5587f259c&gt;</span> in <span class="ni">&lt;module&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">second_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">&lt;ipython-input-44-4a29866e6538&gt;</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>         <span class="c1"># define the sequence of operations in the network including e.g. activations</span>
<span class="ne">---&gt; </span><span class="mi">12</span>         <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> 
<span class="g g-Whitespace">     </span><span class="mi">14</span>         <span class="k">return</span> <span class="n">x</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1100</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1101</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1102</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1103</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1104</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> 
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">103</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">104</span> 
<span class="g g-Whitespace">    </span><span class="mi">105</span>     <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>

<span class="nn">/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py</span> in <span class="ni">linear</span><span class="nt">(input, weight, bias)</span>
<span class="g g-Whitespace">   </span><span class="mi">1846</span>     <span class="k">if</span> <span class="n">has_torch_function_variadic</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1847</span>         <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1848</span>     <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1849</span> 
<span class="g g-Whitespace">   </span><span class="mi">1850</span> 

<span class="ne">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)
</pre></div>
</div>
</div>
</div>
<p>The error message is quite explicit: you then need also to move the data, both for training and inference to the GPU:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image_tensor</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">second_model</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2747, 12.7591,  0.0000,  0.0000,
         3.2894,  3.1473,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        14.3413,  7.2331,  0.0000,  0.0000], device=&#39;cuda:0&#39;,
       grad_fn=&lt;ReluBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p>The output sits on the GPU and is part of the gradient calcualtion, so if you want to further use it, you will have to pull it from the GPU <strong>and</strong> detach it from gradient calculation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 0.       ,  0.       ,  0.       ,  0.       ,  4.2747416,
       12.759116 ,  0.       ,  0.       ,  3.2893572,  3.1472843,
        0.       ,  0.       ,  0.       ,  0.       ,  0.       ,
        0.       , 14.341337 ,  7.2330832,  0.       ,  0.       ],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Create a network with 4 successive linear layers of size 20,40 and 100 and 2, and ReLU activation after the three first layers. It takes a 2D input.</p></li>
<li><p>With <code class="docutils literal notranslate"><span class="pre">skimage.io</span></code> import the image in <code class="docutils literal notranslate"><span class="pre">.../data/woody_baille.JPG</span></code></p></li>
<li><p>Instantiate your model with the appropriate size</p></li>
<li><p>Run the image through the network and turn the output into a numpy array</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="03-Tensors.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3. Tensor calculations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05-Training.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. Training a network</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Guillaume Witz<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>