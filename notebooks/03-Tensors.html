
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Tensor calculations &#8212; Deep Learning for imaging</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Simple neural net with PyTorch" href="04-Simple_NN.html" />
    <link rel="prev" title="2. Very accessible deep learning" href="02-Simple_inference_vgg16.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Deep Learning for imaging</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Readme.html">
   Deep Learning for imaging
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Basics_image_processing.html">
   1. Basics of image handling and processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-Simple_inference_vgg16.html">
   2. Very accessible deep learning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Tensor calculations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Simple_NN.html">
   4. Simple neural net with PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-Training.html">
   5. Training a network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Data_handling.html">
   6. Handling data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Augmentation.html">
   7. Data augmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Lightning.html">
   8. Simplifying code with PyTorch-Lightning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Classify_drawings.html">
   9. Classification: practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10-Convolutions_draw.html">
   10. Convolutions and rescaling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11-Autoencoder_drawings.html">
   11. Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12-TransferLearning_worms.html">
   12. Transfer learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13-Segmentation.html">
   13. Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Unet.html">
   14. U-net
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="15-Unet_nuclei.html">
   15. Unet applied to nuclei segmentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/03-Tensors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/03-Tensors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-arrays">
   Creating arrays
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indexing-broadcasting-etc">
   Indexing, broadcasting etc.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradients">
   Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sending-tensors-to-a-gpu">
   Sending tensors to a GPU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>3. Tensor calculations</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-arrays">
   Creating arrays
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indexing-broadcasting-etc">
   Indexing, broadcasting etc.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradients">
   Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sending-tensors-to-a-gpu">
   Sending tensors to a GPU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a class="reference external" href="https://colab.research.google.com/github/guiwitz/DLImaging/blob/master/notebooks/03-Tensors.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="tensor-calculations">
<h1>3. Tensor calculations<a class="headerlink" href="#tensor-calculations" title="Permalink to this headline">¶</a></h1>
<p>We have seen in the previous cahtper that we need to use a specific type of array (matrix) in the frame of PyTorch based neural networks, namely tensors. These tensors are very similar to Numpy arrays but with additional specific functionalities needed for deep learning. There is an ongoing effort to make the switch between different array/tensor formats (numpy, tensors, xarray etc.) more transparent in the future, but for the moment let’s briefly explore the PyTorch tensors, accessible from the <code class="docutils literal notranslate"><span class="pre">torch</span></code> module:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="creating-arrays">
<h2>Creating arrays<a class="headerlink" href="#creating-arrays" title="Permalink to this headline">¶</a></h2>
<p>Numpy and Pytorch share a lot of functions and methods so you wont feel completely lost. For example you can create arrays filled with ones:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_array</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">t_array</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1.],
        [1., 1.],
        [1., 1.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">n_array</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1.],
       [1., 1.],
       [1., 1.]])
</pre></div>
</div>
</div>
</div>
<p>You can also find the type of the array with <code class="docutils literal notranslate"><span class="pre">dtype</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;t_array dtype: </span><span class="si">{</span><span class="n">t_array</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n_array dtype: </span><span class="si">{</span><span class="n">n_array</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>t_array dtype: torch.float32
n_array dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Pytorch implements as well many other function to create arrays that are very similar to Numpy. For example random number arrays:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Finally you can easily transform Numpy arrays into Pytorch tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_from_n</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_array</span><span class="p">)</span>
<span class="n">t_from_n</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1.],
        [1., 1.],
        [1., 1.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p>And the reverse is true as well: you can recover a Numpy array from a Pytorch tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_from_n</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1., 1.],
       [1., 1.],
       [1., 1.]])
</pre></div>
</div>
</div>
</div>
<p>Finally, Pytorch tensors are also compatible with Matplotlib, so you can easily have a look at them using e.g. <code class="docutils literal notranslate"><span class="pre">imshow</span></code> for 2D tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">t_random</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/03-Tensors_14_0.png" src="../_images/03-Tensors_14_0.png" />
</div>
</div>
</div>
<div class="section" id="indexing-broadcasting-etc">
<h2>Indexing, broadcasting etc.<a class="headerlink" href="#indexing-broadcasting-etc" title="Permalink to this headline">¶</a></h2>
<p>The powerful logic behind Numpy that allows for a very efficient selection and combination of elements in arrays is also conserved in Pytorch. For example regular indexing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[140, 120,  92, 222, 238, 117, 239,  31, 244,  94],
        [ 38, 105,  49,  43,  73,  40, 107,  35, 253, 169],
        [  6,  40, 119, 192,  57, 124, 153,  36,  89, 149],
        [238, 235,  21, 147,  44, 109, 251, 112, 159, 192],
        [ 45, 192, 224, 233, 175,  86, 152, 110, 183,  63],
        [135,  80,   7, 147, 140, 123, 227, 112,  39, 112],
        [100, 249, 114,  93, 225,   9, 238, 164, 164, 156],
        [123, 125, 226,   7, 209,  20,  60, 142,  45,  40],
        [ 57, 114,  28, 143,  27, 204, 187,  84, 219, 126],
        [ 25,  65, 144, 200,  41,  22, 225, 153, 215,  31]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([140, 120,  92, 222, 238, 117, 239,  31, 244,  94])
</pre></div>
</div>
</div>
</div>
<p>or broadcasting that allows to combine tensors of different but compatible shapes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[193., 242., 134.,  51., 199.],
        [193., 242., 134.,  51., 199.],
        [193., 242., 134.,  51., 199.]])
</pre></div>
</div>
</div>
</div>
<p>We will see that very often we also need to flatten arrays for example to create a fully connected layer in a deep learning network. This can be done in two ways. You can use the <code class="docutils literal notranslate"><span class="pre">flatten</span></code> function/method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([140, 120,  92, 222, 238, 117, 239,  31, 244,  94,  38, 105,  49,  43,
         73,  40, 107,  35, 253, 169,   6,  40, 119, 192,  57, 124, 153,  36,
         89, 149, 238, 235,  21, 147,  44, 109, 251, 112, 159, 192,  45, 192,
        224, 233, 175,  86, 152, 110, 183,  63, 135,  80,   7, 147, 140, 123,
        227, 112,  39, 112, 100, 249, 114,  93, 225,   9, 238, 164, 164, 156,
        123, 125, 226,   7, 209,  20,  60, 142,  45,  40,  57, 114,  28, 143,
         27, 204, 187,  84, 219, 126,  25,  65, 144, 200,  41,  22, 225, 153,
        215,  31])
</pre></div>
</div>
</div>
</div>
<p>Here you can also specify which <em>contiguous</em> dimensions you want to flatten e.g.:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_3d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">t_3d</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[[23, 67, 96, 77],
         [85, 53, 13, 83],
         [91,  8, 23, 88]],

        [[96, 16,  1, 18],
         [96, 96,  6, 55],
         [19,  7, 97, 60]]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">t_3d</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[23, 67, 96, 77, 85, 53, 13, 83, 91,  8, 23, 88],
        [96, 16,  1, 18, 96, 96,  6, 55, 19,  7, 97, 60]])
</pre></div>
</div>
</div>
</div>
<p>The alternative is to use the <code class="docutils literal notranslate"><span class="pre">view</span></code> method, which, if possible, returns only a <code class="docutils literal notranslate"><span class="pre">view</span></code> of the array. You can pass compatible dimensions to reshape the tensor, or simple use <code class="docutils literal notranslate"><span class="pre">-1</span></code> to completely flatten it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[130, 134,  63,  67,  49, 141, 235, 237, 178, 150, 249, 157,  69, 116,
           6, 142, 129, 109,   3, 214],
        [ 34, 219,  23,  11,  48, 229,  65, 220,  87, 139,  68, 211, 130, 197,
         198,  70, 218, 208, 200,  62],
        [ 32, 200, 175, 240, 199,  63, 179, 228, 248, 206, 244, 229,  49, 163,
          69, 170, 226,  98,  49,  84],
        [203, 185, 208,  76, 132,  39, 244, 142, 175, 132, 180,  59,  38, 126,
         216, 253,  70, 231, 129, 202],
        [ 10,  21, 221, 126, 215, 206, 216, 211, 245, 168, 159,   9,  54,  49,
          78, 228,  41, 180,  17, 110]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([130, 134,  63,  67,  49, 141, 235, 237, 178, 150, 249, 157,  69, 116,
          6, 142, 129, 109,   3, 214,  34, 219,  23,  11,  48, 229,  65, 220,
         87, 139,  68, 211, 130, 197, 198,  70, 218, 208, 200,  62,  32, 200,
        175, 240, 199,  63, 179, 228, 248, 206, 244, 229,  49, 163,  69, 170,
        226,  98,  49,  84, 203, 185, 208,  76, 132,  39, 244, 142, 175, 132,
        180,  59,  38, 126, 216, 253,  70, 231, 129, 202,  10,  21, 221, 126,
        215, 206, 216, 211, 245, 168, 159,   9,  54,  49,  78, 228,  41, 180,
         17, 110])
</pre></div>
</div>
</div>
</div>
<p>Since we are dealing with a <code class="docutils literal notranslate"><span class="pre">view</span></code>, if we modify one of the arrays <em>in place</em>, the values in the other arrays are changed as well. This means that this is <strong>not</strong> and independent array but just a shallow-copy. Therefore be <em>careful</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">view_copy</span> <span class="o">=</span> <span class="n">t_random</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">view_copy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[130, 134,  63,  67,  49, 141, 235, 237, 178, 150, 249, 157,  69, 116,
           6, 142, 129, 109,   3, 214],
        [ 34, 219,  23,  11,  48, 229,  65, 220,  87, 139,  68, 211, 130, 197,
         198,  70, 218, 208, 200,  62],
        [ 32, 200, 175, 240, 199,  63, 179, 228, 248, 206, 244, 229,  49, 163,
          69, 170, 226,  98,  49,  84],
        [203, 185, 208,  76, 132,  39, 244, 142, 175, 132, 180,  59,  38, 126,
         216, 253,  70, 231, 129, 202],
        [ 10,  21, 221, 126, 215, 206, 216, 211, 245, 168, 159,   9,  54,  49,
          78, 228,  41, 180,  17, 110]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">view_copy</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_random</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="gradients">
<h2>Gradients<a class="headerlink" href="#gradients" title="Permalink to this headline">¶</a></h2>
<p>To be able to perform backpropagation in Deep Learning networks, we need to be able to calculate all the necessary gradients. This feature is “integrated” into Pytorch arrays directly if we use the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> option. To start with a simple example, let’s define first a variable <span class="math notranslate nohighlight">\(x=1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.]], requires_grad=True)
</pre></div>
</div>
</div>
</div>
<p>Now we let our variable pass through a few simple operations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">**</span><span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
<p>Our last variable that depends initially on <code class="docutils literal notranslate"><span class="pre">x</span></code> is now <code class="docutils literal notranslate"><span class="pre">w</span></code>. We see that <span class="math notranslate nohighlight">\(w = f(z) = f(g(y)) = f(g(h(x))) = k(x)\)</span> with:</p>
<p><span class="math notranslate nohighlight">\(f(z) = 5*z\)</span></p>
<p><span class="math notranslate nohighlight">\(g(y) = y^{3/2}\)</span></p>
<p><span class="math notranslate nohighlight">\(h(x) = 2*x\)</span></p>
<p>If <code class="docutils literal notranslate"><span class="pre">w</span></code> needs to be optimized with respect to the variablex x, following th chain rule, we need to calculate <span class="math notranslate nohighlight">\(k'(x) = f'(g(h(x))*g'(h(x))*h'(x)\)</span></p>
<p><span class="math notranslate nohighlight">\(5 * \frac{3}{2}(2x)^{0.5} * 2\)</span></p>
<p>This complete calculation can simply be performed by calcualting the gradients of w <span class="math notranslate nohighlight">\(dw/dx\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[21.2132]])
</pre></div>
</div>
</div>
</div>
<p>We can verify that we indeed obtain the correct gradient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21.213203435596427
</pre></div>
</div>
</div>
</div>
<p>Of course this is an over-simplified example. Calculations become more complex when dealing with actual vectors or tensors but the principle remains the same.</p>
<p>Finally note that if you want to recover a Numpy array from a PyTorch tensor, or plot a PyTorch tensor with Matplotlib, you first have to <em>detach</em> it from the gradient calculation system (if necessary) to recover it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-30-2527552080a3&gt;</span> in <span class="ni">&lt;module&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="ne">RuntimeError</span>: Can&#39;t call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="sending-tensors-to-a-gpu">
<h2>Sending tensors to a GPU<a class="headerlink" href="#sending-tensors-to-a-gpu" title="Permalink to this headline">¶</a></h2>
<p>If your computer is equipped with a compatible GPU or if you run the notebook on Google Colab with a GPU runtime, you can exploit Graphics card computing power. For that the data have to be “pushed” and “pulled” to and from that device. We will see later that we can push entire networks thre but for the moment we just send a tensor.</p>
<p>First we have to check wheter a GPU is available:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>If yes we can device a GPU device (a CUDA device in fact):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">dev</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>device(type=&#39;cuda&#39;)
</pre></div>
</div>
</div>
</div>
<p>Finally we can send the data the the “CUDA” device:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">mytensor</span> <span class="o">=</span> <span class="n">mytensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
<span class="n">mytensor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.1078, -1.2955,  0.9824, -2.2888, -0.9240],
        [ 1.1793, -1.8081, -1.4458, -1.2130,  0.9562],
        [-0.5964,  1.9136, -1.2986, -0.1035,  1.0607]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="nn">&lt;ipython-input-41-aa7480be9066&gt;</span> in <span class="ni">&lt;module&gt;</span><span class="nt">()</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">mytensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="ne">TypeError</span>: can&#39;t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</pre></div>
</div>
</div>
</div>
<p>We see here that we have again difficulties getting the tensor “out” of PyTorch. This time not because it’s part of a gradient but because it lives on the GPU. So we need to first copy it back to the CPU first:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_CPU</span> <span class="o">=</span> <span class="n">mytensor</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mytensor_CPU</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.10778594, -1.2954801 ,  0.98242337, -2.2888114 , -0.9239933 ],
       [ 1.1793374 , -1.8080784 , -1.4457537 , -1.2130216 ,  0.9561631 ],
       [-0.59640384,  1.9136024 , -1.2985845 , -0.10350052,  1.0606741 ]],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>You will regularly hit this kind of problems when writing your code, so remember these two potential issues when you want to <em>post-process</em> some tensor:</p>
<ul class="simple">
<li><p>you migth need to <em>detach</em> it from the gradient calculation</p></li>
<li><p>you migth need to pull it out of the GPU</p></li>
<li><p>for NN computation, you might need to push your data (tensors) to the GPU</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Create a tensor of integers in the range 0-100 of size 16x16</p></li>
<li><p>Change its “gradient-status” by attaching it to gradient calculation</p></li>
<li><p>Solve the problem appearing in (2.) by creating a float32 tensor and attaching the gradient again</p></li>
<li><p>Flatten the array to 1d</p></li>
<li><p>Transform your flat tensor to a numpy array</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="02-Simple_inference_vgg16.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">2. Very accessible deep learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="04-Simple_NN.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">4. Simple neural net with PyTorch</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Guillaume Witz<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>