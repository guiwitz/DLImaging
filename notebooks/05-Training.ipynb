{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/guiwitz/DLImaging/blob/master/notebooks/05-Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create a network, pass an input and an output, we only need to learn how to proceed for training and using it. Let's remember the different steps needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/guiwitz/DLImaging/raw/master/illustrations/ML_principle.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py', 'check_colab.py')\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)\n",
    "\n",
    "Image(url='https://github.com/guiwitz/DLImaging/raw/master/illustrations/ML_principle.jpg',width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we will pass training examples *forward* through the network\n",
    "2. We measure an error between prediction and true label, the loss\n",
    "3. We calculate the gradient of the loss respective to each parameter in the model. This is done by *backpropagation*\n",
    "4. We adjust the parameters using the calculated gradient and an optimizer (e.g. SGD)\n",
    "\n",
    "Additionally we will also see in this notebooks additional aspects such as training epochs and validation. The goal here is to once see the whole pipeline in detail before we start using tools that reduce some of the boiler-plate code necessary here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batches\n",
    "\n",
    "Before we create our network and define a loss, let's remember how training samples are passed through the network. In principle we want to do each optimization step for the **entire dataset** not just a single image as training would have a difficult time to converge. However this is usually not possible and and instead what is generally done is to use **mini-batches**, i.e. the network is iteratively trained on subsets of traininig examples. So now instead of using the gradients produced by a single image, one can use for example the average gradients over the mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/guiwitz/DLImaging/raw/master/illustrations/batch_processing.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://github.com/guiwitz/DLImaging/raw/master/illustrations/batch_processing.jpg',width=700) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is in fact designed to handle batches *by default*. We can see that if we look at the documentation of modules such as ```nn.Linear``` which says that inputs should have the shape ```N x ...``` where ```N``` stands for batch size and ```...``` for other dimensions such as channels, samples etc. This applies in fact to all modules, including those calculating losses. We can therefore feed examples with dimensions ```N x ...``` and PyTorch handles batch calculations for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "What does this mean for out network? We only have to make one slight modification. We used ```x.view(-1)``` previously to flatten 32x32 images into vectors of 1024 elements. If we now feed a batch of size Nx32x32, this would generate a long vector of size Nx1024. So we need to adjust the ```view()``` command and specify the size of the first dimension. In such a way only the image dimensions are flattened: ```x.view(batchsize, -1)```. Alternatively we can use ```torch.flatten(start_dim = 1)``` specifying from which dimension we want to start flattening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_categories):\n",
    "        super(Mynetwork, self).__init__()\n",
    "        \n",
    "        # define e.g. layers here e.g.\n",
    "        self.layer1 = nn.Linear(input_size, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, num_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # define the sequence of operations in the network including e.g. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we instantiate it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mynetwork(1024, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that inputs/outputs work as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myinput = torch.randn((5,32,32))\n",
    "myinput.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myoutput = model(myinput)\n",
    "myoutput.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to pass a single element, e.g. in the inference phase, then we still have to reshape it so that it has dimensions ```N x ...```. The first dimension will just have a size of 1. The simples to do that is to use ```unsqueeze()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myimage = torch.randn((32,32))\n",
    "myimage.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myimage = myimage.unsqueeze(0)\n",
    "myimage.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(myimage)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a loss function and and backpropagating\n",
    "\n",
    "In this example, we are going to classify images. Therefore we can use a standard loss function like cross-entropy which is also available in the ```torch.nn``` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss function is also a module i.e. it is differentiable and can just be integrated in the network. Also it sticks to the same \"batch-logic\" as the other layers. Therefore it expects inputs whose dimensions start with ```N``` for bactches. What we need here is the output of the network of size ```N x C``` where C is the number of categories (2 in our example) and a list of target labels (\"true\" labels) which have of course to be turned into a tensor.\n",
    "\n",
    "We make up some data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysample = torch.randn(3, 32*32)\n",
    "mylabels = torch.tensor([0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass them through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(mysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare output to target with the cross-entropy module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, mylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ```CrossEntropyLoss``` module **automatically** applies soft-max to the output and then calculates the loss. So we **don't need** to have a soft-max layer at the end of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done the forward pass, we can calculate the gradients of the loss by backpropagation. This is simply done by calling the ```backward``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Now that we have an estimate of the loss and gradients, we can optimize all our paramters by using some optimization algorithm. Several are available in ```torch.optim```. We use here the Adam optimizer, one of the \"safest\" choices. As arguments we need to pass a list of parameters that need to be optimized. We can do that by recovering them from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.0096e-02,  3.0674e-05,  2.6671e-02,  ...,  6.5770e-03,\n",
       "           7.3955e-03, -1.8081e-02],\n",
       "         [-1.7888e-02, -1.9279e-02, -1.7028e-02,  ..., -3.8475e-03,\n",
       "          -1.7462e-02,  1.4501e-02],\n",
       "         [ 1.5343e-02, -9.3881e-03, -9.9786e-03,  ...,  1.6292e-03,\n",
       "          -3.2812e-03,  6.1001e-03],\n",
       "         ...,\n",
       "         [ 1.5522e-02,  4.7774e-03, -3.8563e-03,  ..., -4.9394e-03,\n",
       "           1.1973e-02,  2.9910e-02],\n",
       "         [-1.1413e-02, -1.0850e-02, -2.5945e-02,  ..., -1.1821e-02,\n",
       "           1.0397e-02,  2.5489e-02],\n",
       "         [-2.7788e-02, -1.0083e-02,  1.9031e-02,  ..., -7.2356e-03,\n",
       "           2.0071e-02,  1.1560e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 7.7209e-03,  2.4932e-02, -1.4238e-02, -1.6634e-02, -2.8935e-02,\n",
       "          1.2766e-02, -2.0776e-02,  2.9260e-02, -1.4907e-02, -8.9818e-04,\n",
       "          1.9231e-03, -6.5028e-03, -2.9427e-02,  8.3867e-04,  1.2105e-02,\n",
       "          1.1981e-02,  1.7196e-02,  2.0736e-02, -9.3799e-03,  2.4864e-02,\n",
       "         -2.0869e-02, -7.5096e-03, -2.0373e-02,  1.9844e-02, -1.2224e-02,\n",
       "         -1.2027e-02, -2.0100e-02, -9.6672e-03, -2.6220e-02,  2.5552e-02,\n",
       "         -2.4417e-02,  1.5736e-02,  6.9730e-03,  9.4496e-03, -2.4207e-02,\n",
       "         -1.1894e-02,  1.4290e-02, -1.7734e-02, -8.8290e-03, -2.9528e-02,\n",
       "          2.4942e-02,  2.5130e-02,  1.8410e-02, -2.0887e-02, -1.3651e-02,\n",
       "         -2.9620e-02,  4.7190e-03,  1.7522e-02, -4.3134e-03, -6.0331e-05,\n",
       "          2.2341e-02, -1.7073e-02, -1.0343e-03, -2.9692e-02, -3.5632e-03,\n",
       "         -3.8227e-03, -2.8373e-04,  1.0603e-04, -2.3703e-02,  1.5122e-02,\n",
       "         -2.6131e-02, -1.7449e-02,  1.6811e-02, -2.3028e-02, -2.5156e-02,\n",
       "         -5.6807e-03,  5.9528e-03,  6.3489e-03,  5.4543e-03,  9.7150e-03,\n",
       "          1.1492e-02,  1.9453e-03,  1.1023e-02,  1.3774e-02, -1.6266e-02,\n",
       "         -2.1876e-02, -1.5379e-02,  2.2677e-02, -1.2994e-02,  8.6572e-03,\n",
       "          1.1294e-02,  2.4407e-02,  4.7117e-03, -9.9618e-03,  5.7779e-03,\n",
       "         -8.9867e-03, -1.1312e-02, -8.6595e-03, -3.3287e-03,  5.9114e-03,\n",
       "          1.2269e-02,  2.5026e-02, -2.0863e-03,  2.6398e-02, -1.0442e-02,\n",
       "         -2.4469e-02,  1.0880e-02,  7.5141e-03, -3.0736e-02,  8.9216e-03],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0615, -0.0570, -0.0115,  ..., -0.0404,  0.0366,  0.0340],\n",
       "         [-0.0424, -0.0430,  0.0217,  ..., -0.0945,  0.0753,  0.0345],\n",
       "         [-0.0599,  0.0104, -0.0168,  ...,  0.0150, -0.0543,  0.0745],\n",
       "         ...,\n",
       "         [-0.0336,  0.0955,  0.0475,  ..., -0.0381,  0.0819,  0.0184],\n",
       "         [-0.0674, -0.0082, -0.0029,  ..., -0.0161,  0.0626,  0.0039],\n",
       "         [ 0.0575,  0.0464, -0.0442,  ...,  0.0222,  0.0034, -0.0486]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0932, -0.0233,  0.0184,  0.0530, -0.0931, -0.0462, -0.0232, -0.0183,\n",
       "         -0.0833,  0.0932,  0.0846, -0.0992, -0.0546,  0.0305, -0.0917, -0.0055,\n",
       "         -0.0385, -0.0890, -0.0762,  0.0310,  0.0253, -0.0023,  0.0398,  0.0621,\n",
       "          0.0687,  0.0944,  0.0911, -0.0956, -0.0266, -0.0562,  0.0593,  0.0301,\n",
       "         -0.0865,  0.0297,  0.0100,  0.0867,  0.0305,  0.0568, -0.0235, -0.0760,\n",
       "          0.0893, -0.0569, -0.0389, -0.0358,  0.0664,  0.0300,  0.0740, -0.0896,\n",
       "         -0.0177,  0.0874, -0.0278, -0.0761,  0.0530, -0.0306, -0.0100, -0.0776,\n",
       "         -0.0499,  0.0141,  0.0993, -0.0278, -0.0154, -0.0650,  0.0786, -0.0099,\n",
       "         -0.0715,  0.0928,  0.0040, -0.0776,  0.0091,  0.0153, -0.0486,  0.0619,\n",
       "         -0.0576, -0.0776, -0.0790,  0.0824,  0.0067, -0.0238,  0.0267, -0.0023,\n",
       "         -0.0668, -0.0177, -0.0162,  0.0846, -0.0232, -0.0740,  0.0266, -0.0151,\n",
       "          0.0619, -0.0546, -0.0786, -0.0685, -0.0426,  0.0915,  0.0103, -0.0384,\n",
       "          0.0472,  0.0344, -0.0467,  0.0755], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 6.5606e-02, -5.0641e-02, -4.4553e-02, -6.8522e-02, -8.6985e-02,\n",
       "           2.3300e-02,  6.7255e-02, -7.6551e-02,  5.5659e-02, -2.4992e-02,\n",
       "          -6.8363e-02, -8.3316e-02, -3.2795e-03, -5.9428e-02,  6.5071e-02,\n",
       "          -3.3140e-02, -8.5052e-03,  3.7094e-02,  7.8014e-02,  5.7353e-02,\n",
       "          -7.2546e-02,  3.4763e-02, -9.0812e-02, -7.0710e-02,  7.8074e-02,\n",
       "           7.8433e-02, -1.6284e-02, -4.8971e-02, -5.2098e-03,  8.6665e-02,\n",
       "           5.2955e-02, -9.1893e-02, -9.1107e-02, -4.5895e-02, -7.1283e-02,\n",
       "           6.2609e-02,  4.2793e-02,  1.7321e-02, -5.3715e-02, -9.8614e-03,\n",
       "          -1.7231e-02, -6.1988e-02, -8.1160e-02,  5.0111e-02,  5.1918e-02,\n",
       "           9.0604e-02,  3.3860e-02,  4.9688e-02,  2.5078e-03,  7.8077e-02,\n",
       "          -2.7477e-02,  2.2147e-02, -4.4922e-04,  5.8734e-02, -2.4451e-02,\n",
       "           1.2447e-02,  5.7344e-02, -6.0107e-02, -8.1746e-02, -7.7935e-02,\n",
       "          -1.3861e-02,  2.9694e-02, -9.9264e-02, -7.1636e-02, -6.5192e-02,\n",
       "           6.3430e-02, -5.7641e-02,  1.6758e-02,  4.7391e-02, -4.9296e-02,\n",
       "           7.9114e-02, -9.3657e-02,  5.1332e-02, -1.8713e-02,  6.8710e-02,\n",
       "           7.6282e-02,  1.9291e-03,  4.3912e-02, -7.9793e-03,  7.4472e-03,\n",
       "          -6.1379e-02, -9.1564e-02,  4.4773e-04,  9.1251e-03,  7.5297e-02,\n",
       "          -8.9551e-02,  3.7104e-02, -8.5293e-02, -9.1996e-02, -3.1305e-02,\n",
       "          -4.3062e-02,  4.3459e-02, -1.7309e-02,  4.0477e-02,  4.9067e-03,\n",
       "          -3.0141e-02, -3.8705e-02, -5.2362e-02,  7.1610e-02, -9.3980e-02],\n",
       "         [ 7.1125e-02, -7.4714e-02, -8.5683e-02, -4.5036e-02, -1.8454e-02,\n",
       "           6.1750e-02,  9.1406e-02,  3.2390e-02,  6.7827e-02, -5.6413e-02,\n",
       "          -1.8700e-02, -4.1599e-02, -7.5128e-02, -3.4962e-03,  2.8468e-02,\n",
       "           3.4724e-02, -3.3955e-02, -6.7575e-02, -4.4694e-02,  3.2546e-02,\n",
       "           4.3997e-02, -3.4262e-02,  1.5453e-02, -7.2631e-02, -7.9618e-02,\n",
       "          -7.7813e-03, -9.9209e-03,  5.6433e-02,  5.9126e-02,  6.1921e-02,\n",
       "           3.8086e-02, -5.2365e-03,  5.0617e-02,  9.0606e-02,  2.2304e-02,\n",
       "          -5.9386e-02,  7.2383e-02,  2.4626e-03, -8.7539e-02,  3.5524e-02,\n",
       "          -7.5824e-02,  2.2815e-02,  4.0460e-02,  1.6408e-02, -5.0570e-02,\n",
       "           6.2677e-02, -4.8802e-02, -9.8503e-02,  7.4579e-02,  8.9813e-02,\n",
       "          -5.0746e-02,  4.9069e-02,  9.5946e-02, -5.6060e-02, -9.7131e-03,\n",
       "           8.0132e-02,  8.1806e-02, -2.5703e-02,  4.2433e-02,  3.6052e-03,\n",
       "          -9.3600e-03,  6.4871e-02, -3.1354e-03,  3.8400e-02, -3.7732e-02,\n",
       "          -4.8844e-02, -7.6407e-02, -8.4311e-02,  6.1517e-03,  4.5203e-05,\n",
       "          -2.6324e-03, -2.6466e-02,  6.2267e-02, -3.5270e-02, -5.3489e-02,\n",
       "           3.8019e-02,  1.2417e-02, -6.7351e-02, -1.4603e-02, -2.5690e-02,\n",
       "           5.4122e-02,  1.8533e-02, -4.6947e-02, -9.4890e-02,  5.9438e-02,\n",
       "           3.8490e-02,  8.5699e-02, -7.5382e-02, -6.9624e-03, -2.8783e-02,\n",
       "          -4.3161e-02, -4.9431e-02, -4.7825e-03,  9.3961e-03, -9.1059e-02,\n",
       "          -3.5627e-02, -8.6812e-02,  5.4014e-02,  3.1240e-02, -8.4004e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0204, -0.0083], requires_grad=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to acutally do one step of optimization using the ```step``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that some parameters have really changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 2.0096e-02,  3.0674e-05,  2.6671e-02,  ...,  6.5770e-03,\n",
       "           7.3955e-03, -1.8081e-02],\n",
       "         [-1.6888e-02, -1.8279e-02, -1.6028e-02,  ..., -4.8475e-03,\n",
       "          -1.8462e-02,  1.5501e-02],\n",
       "         [ 1.4343e-02, -1.0388e-02, -8.9786e-03,  ...,  2.6292e-03,\n",
       "          -2.2812e-03,  5.1001e-03],\n",
       "         ...,\n",
       "         [ 1.4522e-02,  5.7774e-03, -2.8563e-03,  ..., -3.9394e-03,\n",
       "           1.2973e-02,  3.0910e-02],\n",
       "         [-1.2413e-02, -1.1850e-02, -2.6945e-02,  ..., -1.0821e-02,\n",
       "           9.3967e-03,  2.6489e-02],\n",
       "         [-2.8788e-02, -1.1083e-02,  1.8031e-02,  ..., -6.2356e-03,\n",
       "           1.9071e-02,  1.0560e-02]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 7.7209e-03,  2.5932e-02, -1.3238e-02, -1.5634e-02, -2.9935e-02,\n",
       "          1.3766e-02, -1.9776e-02,  3.0260e-02, -1.5907e-02, -1.8982e-03,\n",
       "          9.2313e-04, -5.5028e-03, -2.9427e-02, -1.6133e-04,  1.3105e-02,\n",
       "          1.0981e-02,  1.6196e-02,  1.9736e-02, -8.3799e-03,  2.4864e-02,\n",
       "         -1.9869e-02, -7.5096e-03, -1.9373e-02,  1.9844e-02, -1.2224e-02,\n",
       "         -1.2027e-02, -1.9100e-02, -8.6672e-03, -2.6220e-02,  2.4552e-02,\n",
       "         -2.3417e-02,  1.6736e-02,  6.9730e-03,  1.0450e-02, -2.3207e-02,\n",
       "         -1.2894e-02,  1.4290e-02, -1.8734e-02, -8.8290e-03, -2.8528e-02,\n",
       "          2.3942e-02,  2.4130e-02,  1.7410e-02, -2.1887e-02, -1.3651e-02,\n",
       "         -2.8620e-02,  3.7190e-03,  1.6522e-02, -3.3134e-03, -1.0603e-03,\n",
       "          2.3341e-02, -1.8073e-02, -3.4312e-05, -3.0692e-02, -2.5632e-03,\n",
       "         -2.8227e-03,  7.1627e-04,  1.0603e-04, -2.2703e-02,  1.6122e-02,\n",
       "         -2.7131e-02, -1.6449e-02,  1.5811e-02, -2.4028e-02, -2.6156e-02,\n",
       "         -6.6807e-03,  6.9528e-03,  5.3489e-03,  6.4543e-03,  1.0715e-02,\n",
       "          1.1492e-02,  2.9453e-03,  1.0023e-02,  1.3774e-02, -1.5266e-02,\n",
       "         -2.0876e-02, -1.5379e-02,  2.3677e-02, -1.1994e-02,  9.6572e-03,\n",
       "          1.2294e-02,  2.5407e-02,  5.7117e-03, -9.9618e-03,  4.7779e-03,\n",
       "         -7.9867e-03, -1.2312e-02, -7.6595e-03, -2.3288e-03,  4.9114e-03,\n",
       "          1.1269e-02,  2.4026e-02, -3.0863e-03,  2.7398e-02, -1.0442e-02,\n",
       "         -2.3475e-02,  1.0880e-02,  8.5141e-03, -2.9736e-02,  9.9216e-03],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0615, -0.0570, -0.0115,  ..., -0.0404,  0.0366,  0.0340],\n",
       "         [-0.0424, -0.0440,  0.0207,  ..., -0.0955,  0.0753,  0.0335],\n",
       "         [-0.0599,  0.0104, -0.0168,  ...,  0.0150, -0.0543,  0.0745],\n",
       "         ...,\n",
       "         [-0.0336,  0.0965,  0.0485,  ..., -0.0371,  0.0809,  0.0174],\n",
       "         [-0.0674, -0.0092, -0.0039,  ..., -0.0171,  0.0626,  0.0039],\n",
       "         [ 0.0575,  0.0474, -0.0432,  ...,  0.0232,  0.0024, -0.0496]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0932, -0.0243,  0.0184,  0.0540, -0.0941, -0.0452, -0.0242, -0.0173,\n",
       "         -0.0823,  0.0942,  0.0836, -0.1002, -0.0556,  0.0305, -0.0917, -0.0055,\n",
       "         -0.0385, -0.0890, -0.0762,  0.0300,  0.0253, -0.0033,  0.0398,  0.0621,\n",
       "          0.0697,  0.0944,  0.0921, -0.0946, -0.0276, -0.0552,  0.0603,  0.0311,\n",
       "         -0.0865,  0.0307,  0.0090,  0.0857,  0.0315,  0.0578, -0.0225, -0.0770,\n",
       "          0.0903, -0.0569, -0.0399, -0.0348,  0.0654,  0.0310,  0.0730, -0.0886,\n",
       "         -0.0177,  0.0874, -0.0278, -0.0751,  0.0540, -0.0306, -0.0100, -0.0786,\n",
       "         -0.0489,  0.0141,  0.1003, -0.0288, -0.0154, -0.0640,  0.0776, -0.0089,\n",
       "         -0.0705,  0.0918,  0.0030, -0.0786,  0.0081,  0.0143, -0.0496,  0.0629,\n",
       "         -0.0566, -0.0766, -0.0800,  0.0814,  0.0057, -0.0248,  0.0277, -0.0013,\n",
       "         -0.0658, -0.0167, -0.0172,  0.0846, -0.0232, -0.0740,  0.0276, -0.0161,\n",
       "          0.0629, -0.0536, -0.0796, -0.0675, -0.0416,  0.0925,  0.0103, -0.0394,\n",
       "          0.0462,  0.0354, -0.0477,  0.0765], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0656, -0.0516, -0.0446, -0.0675, -0.0880,  0.0223,  0.0683, -0.0776,\n",
       "           0.0547, -0.0240, -0.0694, -0.0823, -0.0043, -0.0594,  0.0651, -0.0331,\n",
       "          -0.0085,  0.0371,  0.0780,  0.0564, -0.0725,  0.0338, -0.0908, -0.0707,\n",
       "           0.0771,  0.0784, -0.0153, -0.0500, -0.0042,  0.0877,  0.0540, -0.0929,\n",
       "          -0.0911, -0.0469, -0.0723,  0.0616,  0.0418,  0.0183, -0.0547, -0.0089,\n",
       "          -0.0182, -0.0620, -0.0802,  0.0511,  0.0509,  0.0896,  0.0329,  0.0507,\n",
       "           0.0025,  0.0781, -0.0275,  0.0211, -0.0014,  0.0587, -0.0245,  0.0114,\n",
       "           0.0563, -0.0601, -0.0827, -0.0769, -0.0139,  0.0287, -0.1003, -0.0726,\n",
       "          -0.0662,  0.0624, -0.0586,  0.0158,  0.0464, -0.0483,  0.0781, -0.0947,\n",
       "           0.0503, -0.0197,  0.0677,  0.0773,  0.0029,  0.0429, -0.0090,  0.0084,\n",
       "          -0.0624, -0.0906, -0.0006,  0.0091,  0.0753, -0.0896,  0.0361, -0.0843,\n",
       "          -0.0910, -0.0323, -0.0441,  0.0445, -0.0183,  0.0395,  0.0049, -0.0311,\n",
       "          -0.0397, -0.0534,  0.0706, -0.0950],\n",
       "         [ 0.0711, -0.0737, -0.0857, -0.0460, -0.0175,  0.0627,  0.0904,  0.0334,\n",
       "           0.0688, -0.0574, -0.0177, -0.0426, -0.0741, -0.0035,  0.0285,  0.0347,\n",
       "          -0.0340, -0.0676, -0.0447,  0.0335,  0.0440, -0.0333,  0.0155, -0.0726,\n",
       "          -0.0786, -0.0078, -0.0109,  0.0574,  0.0581,  0.0609,  0.0371, -0.0042,\n",
       "           0.0506,  0.0916,  0.0233, -0.0584,  0.0734,  0.0015, -0.0865,  0.0345,\n",
       "          -0.0748,  0.0228,  0.0395,  0.0154, -0.0496,  0.0637, -0.0478, -0.0995,\n",
       "           0.0746,  0.0898, -0.0507,  0.0501,  0.0969, -0.0561, -0.0097,  0.0811,\n",
       "           0.0828, -0.0257,  0.0434,  0.0026, -0.0094,  0.0659, -0.0021,  0.0394,\n",
       "          -0.0367, -0.0478, -0.0754, -0.0833,  0.0072, -0.0010, -0.0016, -0.0255,\n",
       "           0.0633, -0.0343, -0.0525,  0.0370,  0.0114, -0.0664, -0.0136, -0.0267,\n",
       "           0.0551,  0.0175, -0.0459, -0.0949,  0.0594,  0.0385,  0.0867, -0.0764,\n",
       "          -0.0080, -0.0278, -0.0422, -0.0504, -0.0038,  0.0104, -0.0911, -0.0346,\n",
       "          -0.0858,  0.0550,  0.0322, -0.0830]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0214, -0.0073], requires_grad=True)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "We use the cross-entropy as loss because it allows us to optimize our network. However what we are ultimately interested in is the **accuracy** of our model i.e. whether the correct label has been found or not. Such a binary answer is not useful for optimization but is what we want to monitor in the end. Let's generate some random data and see how we can calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myimages = torch.randn((3,32,32))\n",
    "labels = torch.randint(0,2,(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0419, -0.1089],\n",
       "        [-0.3458, -0.0347],\n",
       "        [-0.2012,  0.0032]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(myimages)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted category is the one with the highest probability (not normalized here but it doesn't matter). We can therefore just look for the index of the maximum value along the horizontal dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare prediction and true label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels == output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the sum over this tensor, it tells us how many samples in the batch were correctly predicted and the average accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3333)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels == output.argmax(dim=1)).sum()/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now that we know that all steps work, we want to test our network. We will create a synthetic dataset for that using ```skimage.draw```. We will just generate random images with either circles or triangles. As we have an \"infinite\" amount of data available, we just artificially set a size for our dataset.\n",
    "\n",
    "Don't forget that we need a training and a validation dataset. Every time we have trained the network with the whole training dataset we check prediction quality with the validation dataset to make sure e.g. we are not over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import random_shapes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes as input a keyword - ```triangle```, ```circle```, etc. - and outputs an image with that object as a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(shape, imsize):\n",
    "    \"\"\"Generate image of given shape scaled 0-1.\n",
    "    shape: str\n",
    "        shape to draw (circle, triangle, rectangle)\n",
    "    imsize: int\n",
    "        size of image\n",
    "    \"\"\"\n",
    "    \n",
    "    image, _ = random_shapes(\n",
    "        image_shape=(imsize,imsize),max_shapes=1, min_shapes=1,\n",
    "        num_channels=1, channel_axis=None, shape=shape, min_size=8)\n",
    "    #normalize\n",
    "    image = (255-image)/255\n",
    "\n",
    "    # turn into tensor\n",
    "    image_tensor = torch.tensor(image,dtype=torch.float32)\n",
    "    \n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create pairs of images and labels we simply create a list of possible shapes and randomly pick values from there. We should also not forget to transform the label into a tensor. Let's make a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_type = ['circle', 'triangle','rectangle']\n",
    "num_cat = len(im_type)\n",
    "image_size = 32\n",
    "\n",
    "label = torch.randint(0,num_cat,(1,))\n",
    "image = make_image(im_type[label], image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image)\n",
    "ax.set_title(im_type[label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train our network using mini-batches, and each batch should have a size ```N x H x W``` where ```N``` is the batch size and ```H,W``` the image dimension. We can create a batch by stacking multiple 2D tensors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "single_batch = torch.stack([make_image('circle', image_size) for x in range(batch_size)])\n",
    "single_batch.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we want to mix the different image types, so we generate labels of size ```batch_size``` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = torch.randint(0,num_cat,(batch_size,))\n",
    "single_batch = torch.stack([make_image(im_type[x], image_size) for x in label])\n",
    "\n",
    "fig, ax = plt.subplots(1,4)\n",
    "for x in range(4):\n",
    "    ax[x].imshow(single_batch[x,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our full dataset (as a list of tensors) by generating ```M``` bachtes in order to have ```M x N``` training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of training dataset\n",
    "batch_size = 10\n",
    "training_size = 5000\n",
    "validation_size = 100\n",
    "train_batch_number = int(training_size/batch_size)\n",
    "validation_batch_number = int(validation_size/batch_size)\n",
    "\n",
    "training_label = [torch.randint(0,num_cat,(batch_size,)) for i in range(train_batch_number)]\n",
    "validation_label = [torch.randint(0,num_cat,(batch_size,)) for i in range(validation_batch_number)]\n",
    "\n",
    "train_batches = [torch.stack([make_image(im_type[x], image_size) for x in lab]) for lab in training_label]\n",
    "valid_batches = [torch.stack([make_image(im_type[x], image_size) for x in lab]) for lab in validation_label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Now we can create a loop where we iterate through our batches to train our network and go through the steps defined above. We will do two loops: \n",
    "1. Over epochs: one epoch representing a training step over **all batches**\n",
    "2. Over batches\n",
    "\n",
    "We do validation only once per epoch to see how training goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "valid_accuracy: 0.7099999189376831\n",
      "epoch: 1\n",
      "valid_accuracy: 0.7500000596046448\n",
      "epoch: 2\n",
      "valid_accuracy: 0.7800000309944153\n",
      "epoch: 3\n",
      "valid_accuracy: 0.8399999737739563\n",
      "epoch: 4\n",
      "valid_accuracy: 0.8500000238418579\n",
      "epoch: 5\n",
      "valid_accuracy: 0.9199999570846558\n",
      "epoch: 6\n",
      "valid_accuracy: 0.8899999856948853\n",
      "epoch: 7\n",
      "valid_accuracy: 0.9199999570846558\n",
      "epoch: 8\n",
      "valid_accuracy: 0.9199999570846558\n",
      "epoch: 9\n",
      "valid_accuracy: 0.9199999570846558\n"
     ]
    }
   ],
   "source": [
    "#del model\n",
    "model = Mynetwork(1024, 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    # initialize running accuracy\n",
    "    running_accuracy = 0\n",
    "    \n",
    "    for t in range(train_batch_number):\n",
    "    \n",
    "        # get batch\n",
    "        label = training_label[t]\n",
    "        mybatch = train_batches[t]\n",
    "\n",
    "        # calculate predicted label and calculate loss\n",
    "        pred = model(mybatch)\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # do the optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # set gradients to zero as PyTorch accumulates them otherwise\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate accuracy\n",
    "        mean_accuracy = (torch.argmax(pred,dim=1) == label).sum()/batch_size\n",
    "        running_accuracy+=mean_accuracy\n",
    "\n",
    "        every_nth = 1000\n",
    "        if t % every_nth == every_nth-1: \n",
    "            print(f'accuracy: {running_accuracy/every_nth}')\n",
    "            running_accuracy = 0.0\n",
    "    \n",
    "    # validation\n",
    "    valid_accuracy = 0\n",
    "    for t in range(validation_batch_number):\n",
    "    \n",
    "        # get batch\n",
    "        label = validation_label[t]\n",
    "        mybatch = valid_batches[t]\n",
    "\n",
    "        # calculate predicted label\n",
    "        pred = model(mybatch)\n",
    "        # calculate accuracy\n",
    "        mean_accuracy = (torch.argmax(pred,dim=1) == label).sum()/batch_size\n",
    "        valid_accuracy += mean_accuracy\n",
    "    valid_accuracy = valid_accuracy/validation_batch_number\n",
    "    print(f'valid_accuracy: {valid_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that accuracy improves but not to a perfect level. We can try to increase the number of epochs or the training size. We can also try to understand where the problem is. For example we can try to find out which images are most mis-classified using a confusion matrix from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a test batch and obtain a prediction with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.randint(0,len(im_type),(100,))\n",
    "mybatch = torch.stack([make_image(im_type[x], image_size) for x in label])\n",
    "pred = model(mybatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the maximum index for each element of the batch gives us the final class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2,\n",
       "        0, 1, 2, 1, 2, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 2, 0, 2, 0, 1,\n",
       "        2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 2, 2, 0, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 0,\n",
       "        2, 1, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2,\n",
       "        2, 0, 2, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 2, 1, 0, 1, 1, 1, 0, 2, 2, 2,\n",
       "        0, 1, 2, 1, 2, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1,\n",
       "        2, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 2, 0, 2, 2, 0, 2, 2, 1, 1, 1, 2, 1, 0,\n",
       "        2, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2,\n",
       "        0, 0, 2, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcualte the confusion matrix and transform it into a Dataframe that we can then easily plot with seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(pred.argmax(dim=1), label), index = im_type,\n",
    "                  columns = im_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the problem is mostly the circle. Because we use a tiny image, small circles are not smooth and can look like rectangles or triangles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Re-use the network you have create for the exercise 4. Make sure it can take **batches** of 2D images as input.\n",
    "2. Create an image generator which creates black (value=0) images (2D tensors) with a single white (value=1) line which is vertical (class #1) or horizontal (class #2)\n",
    "3. Create training and validation data generators\n",
    "4. Add a training loop and train. Verify that the nework works\n",
    "5. \"Bonus\": try to add various levels of noise to the image and see how prediction is affected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
